---
title: "AN588_Malfunction_imcarlyy"
date: "2025-04-01"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
    toc: true
    toc_depth: 2
---
```{r}
options(repos = c(CRAN = "https://cran.rstudio.com"))
```

```{r}
# install.packages("curl")
# install.packages("ggplot2")
# install.packages("dplyr")
```

> Is installing the packages like this really necessary? I'd think just loading the libraries up top would be sufficient since it gives a warning if you load an Rmd file without those packages installed. Doing this every time also throws an error since it's trying to reinstall packages that I already have on my R instance. - BP

# Homework 4: What's your Malfunction

## Part 1: Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:

### A) Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test(). 

### B) When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().

### C) The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

### D) The function should contain a check for the rules of thumb we have talked about (n∗p>5  and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.

### E) The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

```{r}
Z.prop.test <- function(p1, n1, p0, p2 = NULL, n2 = NULL, 
                        alternative = "two.sided", conf.level = 0.95) {
  
  # This function is to check if the normal approximation is valid
  check_validity <- function(p, n) {
    if ((n * p < 5) || (n * (1 - p) < 5)) {
      warning("Warning: Normal approximation may not be valid as np or n(1-p) is < 5.")
    }
  }
  
  # Here, we check validity for first sample
  check_validity(p1, n1)
  # If there's a second sample, check validity for it too
  if (!is.null(p2) && !is.null(n2)) check_validity(p2, n2)

  # One-Sample Z-Test: Comparing p1 to expected proportion p0
  if (is.null(p2) || is.null(n2)) { 
    se <- sqrt((p0 * (1 - p0)) / n1)  # Standard Error (SE) formula for one-sample test
    Z <- (p1 - p0) / se  # Z-score formula
  } else {  # Two-Sample Z-Test: Comparing p1 vs p2
    pooled_p <- (p1 * n1 + p2 * n2) / (n1 + n2)  # Pooled proportion
    se <- sqrt(pooled_p * (1 - pooled_p) * (1/n1 + 1/n2))  # Standard Error for two-sample test
    Z <- (p1 - p2) / se  # Z-score formula
  }

  # Here, we compute p-value based on the alternative hypothesis
  if (alternative == "two.sided") {
    P <- 2 * (1 - pnorm(abs(Z)))  # Two-tailed test p-value
  } else if (alternative == "greater") {
    P <- 1 - pnorm(Z)  # Right-tailed test p-value
  } else if (alternative == "less") {
    P <- pnorm(Z)  # Left-tailed test p-value
  } else {
    stop("Invalid alternative hypothesis. Choose 'two.sided', 'greater', or 'less'.")
  }

  # Here, we compute confidence interval (CI)
  z_crit <- qnorm(1 - (1 - conf.level) / 2)  # Get critical Z value for CI
  
  if (is.null(p2) || is.null(n2)) {  # One-sample CI
    CI <- c(p1 - z_crit * sqrt((p1 * (1 - p1)) / n1),
            p1 + z_crit * sqrt((p1 * (1 - p1)) / n1))
  } else {  # Two-sample CI
    CI <- c((p1 - p2) - z_crit * sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2),
            (p1 - p2) + z_crit * sqrt((p1 * (1 - p1)) / n1 + (p2 * (1 - p2)) / n2))
  }

  # Finally, we ask R to return results as a list (Z-score, p-value, confidence interval)
  return(list(Z = Z, P = P, CI = CI))
}


```

> Interesting how you divided up the confidence interval and validity checks. I think I ended up generating the CI values within the if else loops for each sample condition, just so I didn't have to do another loop just for them. Your way definitely improves legibility, though. Also good thinking making the condition validity check a function so you could just call it instead of doing the entire thing fresh for each, I'll think about doing it that way for mine as well! - BP

```{r}
Z.prop.test(0.5,30,0.8, alternative = "less")
```
> testing out the function, looks like it works! - BP

## Part 2: [2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

### A) Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r}
# Load necessary libraries
library(curl)
library(ggplot2)
library(dplyr)

# Download and read the dataset
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
data <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)

# Clean the data by removing rows with missing values (NA)
clean_data <- na.omit(data)

# Fit the first linear model: Longevity ~ Brain Size
model1 <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = clean_data)

# Fit the second linear model: log(Longevity) ~ log(Brain Size)
clean_data$log_MaxLongevity_m <- log(clean_data$MaxLongevity_m)
clean_data$log_Brain_Size <- log(clean_data$Brain_Size_Species_Mean)
model2 <- lm(log_MaxLongevity_m ~ log_Brain_Size, data = clean_data)

# Create the equation for both models
eq1 <- paste0("Longevity = ", round(coef(model1)[1], 2), " + ", round(coef(model1)[2], 2), " * Brain Size")
eq2 <- paste0("log(Longevity) = ", round(coef(model2)[1], 2), " + ", round(coef(model2)[2], 2), " * log(Brain Size)")

# Plot 1: Longevity vs Brain Size (simple linear model)
p1 <- ggplot(clean_data, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(title = "Longevity vs Brain Size",
       x = "Brain Size (grams)",
       y = "Longevity (months)") +
  annotate("text", x = max(clean_data$Brain_Size_Species_Mean) * 0.7, 
           y = max(clean_data$MaxLongevity_m) * 0.9, label = eq1, hjust = 0, size = 5, color = "blue")

# Plot 2: log(Longevity) vs log(Brain Size)
p2 <- ggplot(clean_data, aes(x = log_Brain_Size, y = log_MaxLongevity_m)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "log(Longevity) vs log(Brain Size)",
       x = "log(Brain Size)",
       y = "log(Longevity)") +
  annotate("text", x = max(clean_data$log_Brain_Size) * 0.7, 
           y = max(clean_data$log_MaxLongevity_m) * 0.9, label = eq2, hjust = 0, size = 5, color = "red")

# Print the plots
print(p1)
print(p2)

```

> Looks good, though I'd consider just loading up all your libraries at the top of the document so the chunk doesn't take time loading them down here. - BP

### B) Identify and interpret the point estimate of the slope (β1 ), as well as the outcome of the test associated with the hypotheses H0: β1  = 0; HA: β1  ≠ 0. Also, find a 90 percent CI for the slope (β1 ) parameter.


```{r}
# Extract summary statistics for both models
summary1 <- summary(model1)
summary2 <- summary(model2)

# Extract the slope estimates (β1) for both models
beta1_model1 <- summary1$coefficients[2, 1]  # Slope estimate for the first model
beta1_model2 <- summary2$coefficients[2, 1]  # Slope estimate for the second model

# Extract p-values for hypothesis testing H0: β1 = 0; HA: β1 ≠ 0
p_value_model1 <- summary1$coefficients[2, 4]
p_value_model2 <- summary2$coefficients[2, 4]

# Compute 90% Confidence Intervals for the slope parameter (β1)
ci_90_model1 <- confint(model1, level = 0.90)[2, ]
ci_90_model2 <- confint(model2, level = 0.90)[2, ]

# Print results
cat("Model: Longevity ~ Brain Size\n")
cat("Slope Estimate (β1):", beta1_model1, "\n")
cat("p-value:", p_value_model1, "\n")
cat("90% CI for β1:", ci_90_model1, "\n\n")

cat("Model: log(Longevity) ~ log(Brain Size)\n")
cat("Slope Estimate (β1):", beta1_model2, "\n")
cat("p-value:", p_value_model2, "\n")
cat("90% CI for β1:", ci_90_model2, "\n")

```

> Your slope estimate looks good but I'm not seeing an interpretation of it? - BP

### C) Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

```{r}
# Create a new data frame for predictions
new_data <- data.frame(Brain_Size_Species_Mean = seq(min(clean_data$Brain_Size_Species_Mean, na.rm = TRUE),
                                                     max(clean_data$Brain_Size_Species_Mean, na.rm = TRUE),
                                                     length.out = 100))

# Get predictions and intervals for confidence and prediction intervals
predictions <- predict(model1, newdata = new_data, interval = "confidence", level = 0.90)
predictions_pi <- predict(model1, newdata = new_data, interval = "prediction", level = 0.90)

# Add the predictions and intervals to the new_data
new_data$fit <- predictions[, "fit"]
new_data$lwr_CI <- predictions[, "lwr"]
new_data$upr_CI <- predictions[, "upr"]
new_data$lwr_PI <- predictions_pi[, "lwr"]
new_data$upr_PI <- predictions_pi[, "upr"]

# Plot with confidence and prediction intervals
ggplot(clean_data, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m)) +
  geom_point(alpha = 0.6) +  # Scatter plot of actual data
  geom_line(data = new_data, aes(y = fit, color = "Regression Line"), size = 1) +  # Fitted line
  geom_line(data = new_data, aes(y = lwr_CI, color = "90% Confidence Interval"), linetype = "dashed", size = 1) +
  geom_line(data = new_data, aes(y = upr_CI, color = "90% Confidence Interval"), linetype = "dashed", size = 1) +
  geom_line(data = new_data, aes(y = lwr_PI, color = "90% Prediction Interval"), linetype = "dotted", size = 1) +
  geom_line(data = new_data, aes(y = upr_PI, color = "90% Prediction Interval"), linetype = "dotted", size = 1) +
  labs(title = "Longevity vs Brain Size with Confidence and Prediction Intervals",
       x = "Brain Size (grams)",
       y = "Longevity (months)") +
  scale_color_manual(values = c("Regression Line" = "blue",
                                "90% Confidence Interval" = "red",
                                "90% Prediction Interval" = "green")) +
  theme_minimal() +
  theme(legend.title = element_blank())
```

> I'm getting an error message saying the "size" aesthetic was deprecated and that we're supposed to use "linewidth" now instead? Seems to still be working though! Also for legibility I'd maybe consider using a darker green, or better yet a color-blind sensitive palette! - BP

### D) Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

```{r}
# Define a new species with Brain Size = 800g
new_species <- data.frame(Brain_Size_Species_Mean = 800)

# Get point estimate and 90% Prediction Interval
prediction <- predict(model1, newdata = new_species, interval = "prediction", level = 0.90)

# Display the results
cat("Point Estimate for Longevity (months):", round(prediction[1], 2), "\n")
cat("90% Prediction Interval: [", round(prediction[2], 2), ",", round(prediction[3], 2), "]\n")


```
I would be cautious about fully trusting this prediction because a brain size of 800g is outside the typical range for this dataset. Since it's far from the mean brain size, the model might not be as accurate for predictions at this extreme value.

### E) Looking at your two models, which do you think is better? Why?
I’d go with the log-transformed model (log(Lifespan) ~ log(Brain Mass)) because the R-squared value is higher, which means it’s doing a better job of explaining the variation in longevity. That’s a good sign it fits the data better. But if we’re talking about which one’s more practical, I’d say the linear model (Lifespan ~ Brain Mass) is probably the better option. It's simpler to understand and communicate, especially if you're trying to explain it to someone without a deep stats background. Plus, it predicts lifespan directly in months, which makes it a bit more intuitive.

> I'm inclined to agree with going with the log transformed model being better. While I can see the argument for using the linear model for simplicity's sake, I'm disinclined to sacrifice accuracy and goodness of fit purely for the benefit of popular communication, at least in this case given that log-transforming variables isn't a particularly difficult thing for most people to wrap their head around? I suppose it's somewhat a matter of taste, however. 
